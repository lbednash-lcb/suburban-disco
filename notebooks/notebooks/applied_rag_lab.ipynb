{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c27d14",
   "metadata": {},
   "source": [
    "One-click ‚ÄúReset & Run‚Äù master cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29cc3e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CWD: /workspaces/suburban-disco/notebooks/notebooks\n",
      "Repo root: /workspaces/suburban-disco\n",
      "Env loaded: True\n",
      "Using Top Shelf files: ['The_Top_Shelf_Collection_RAG_with_artist_years_filled.csv']\n",
      "‚úÖ Built 129 Top Shelf records.\n",
      "Example snippet:\n",
      " Artist: 10,000 Maniacs | Title: In My Tribe | Genre: Alternative Rock\n",
      "üî¢ Embedding 129 docs‚Ä¶\n",
      "üìê Embeddings shape: (129, 1536)\n",
      "\n",
      "üß† ANSWER:\n",
      " Notable artist: Simon & Garfunkel | Album: Parsley, Sage, Rosemary and Thyme | Genre: Folk Rock. Distinctive detail: This album features a blend of intricate harmonies and poetic lyrics. \n",
      "\n",
      "üîé TOP HITS:\n",
      "- The_Top_Shelf_Collection_RAG_with_artist_years_filled.csv-118 | score=0.401 | Artist: Various Artists | Title: Woodstock - Music from the Original Soundtrack | Genre: Rock/Folk/Blues‚Ä¶\n",
      "- The_Top_Shelf_Collection_RAG_with_artist_years_filled.csv-74 | score=0.397 | Artist: Simon & Garfunkel | Title: Parsley, Sage, Rosemary and Thyme | Genre: Folk Rock‚Ä¶\n",
      "- The_Top_Shelf_Collection_RAG_with_artist_years_filled.csv-109 | score=0.389 | Artist: The Velvet Underground | Title: Loaded | Genre: Rock/Proto-Punk‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "# --- MASTER CELL: safe re-run after any restart ---\n",
    "# It will:\n",
    "# 1) Detect repo_root, 2) load .env, 3) pick only Top Shelf files,\n",
    "# 4) build records, 5) embed in memory, 6) answer one test query.\n",
    "\n",
    "%pip -q install --upgrade numpy pandas openai python-dotenv tiktoken\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json, pandas as pd, numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "\n",
    "# ---------- 1) detect repo root ----------\n",
    "def detect_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p/\".env\").exists() or (p/\"README.md\").exists() or (p/\"data\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "CWD = Path.cwd()\n",
    "repo_root = detect_repo_root(CWD)\n",
    "print(\"CWD:\", CWD)\n",
    "print(\"Repo root:\", repo_root)\n",
    "\n",
    "# ---------- 2) load .env ----------\n",
    "loaded = load_dotenv(repo_root / \".env\", override=True)\n",
    "print(\"Env loaded:\", loaded)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise SystemExit(\"‚ùå OPENAI_API_KEY not found. Put it in repo_root/.env then re-run this cell.\")\n",
    "\n",
    "# ---------- 3) collect ONLY Top Shelf files ----------\n",
    "data_dir = repo_root / \"data\"\n",
    "if not data_dir.exists():\n",
    "    raise SystemExit(f\"‚ùå data folder not found at {data_dir}. Create it and add your files.\")\n",
    "\n",
    "topshelf_csv  = sorted([p for p in data_dir.glob(\"*.csv\")  if \"top\" in p.name.lower() and \"shelf\" in p.name.lower()])\n",
    "topshelf_json = sorted([p for p in data_dir.glob(\"*.json\") if \"top\" in p.name.lower() and \"shelf\" in p.name.lower()])\n",
    "\n",
    "if not (topshelf_csv or topshelf_json):\n",
    "    print(\"‚ö†Ô∏è No Top Shelf files matched. Showing all data files instead:\")\n",
    "    print([f.name for f in data_dir.iterdir()])\n",
    "    raise SystemExit(\"Rename your files to include 'top' and 'shelf', or adjust the filters in this cell.\")\n",
    "\n",
    "print(\"Using Top Shelf files:\", [p.name for p in (topshelf_json + topshelf_csv)])\n",
    "\n",
    "# ---------- 4) build records ----------\n",
    "records: List[Dict] = []\n",
    "\n",
    "def add_record(text: str, rid: str):\n",
    "    text = (text or \"\").strip()\n",
    "    if text:\n",
    "        records.append({\"id\": rid, \"text\": text})\n",
    "\n",
    "def join_item_fields(item: dict) -> str:\n",
    "    # Prefer explicit text/content\n",
    "    if isinstance(item, dict):\n",
    "        txt = (item.get(\"text\") or item.get(\"content\") or \"\").strip()\n",
    "        if txt:\n",
    "            return txt\n",
    "        # Otherwise join string-like fields\n",
    "        parts = []\n",
    "        for k in sorted(item.keys()):\n",
    "            v = item[k]\n",
    "            if isinstance(v, (str, int, float)) and str(v).strip():\n",
    "                parts.append(f\"{k}: {str(v).strip()}\")\n",
    "        return \" | \".join(parts)\n",
    "    return str(item)\n",
    "\n",
    "# JSON first (if present)\n",
    "for jp in topshelf_json:\n",
    "    data = json.loads(jp.read_text(encoding=\"utf-8\"))\n",
    "    if not isinstance(data, list) or not data:\n",
    "        continue\n",
    "    for i, item in enumerate(data):\n",
    "        add_record(join_item_fields(item), f\"{jp.name}-{i}\")\n",
    "\n",
    "# Then CSVs\n",
    "for cp in topshelf_csv:\n",
    "    try:\n",
    "        df = pd.read_csv(cp)\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = pd.read_csv(cp, encoding=\"utf-8\")\n",
    "        except Exception:\n",
    "            df = pd.read_csv(cp, encoding=\"latin-1\")\n",
    "    # Choose columns (auto if not set)\n",
    "    likely = {\"artist\",\"album\",\"title\",\"description\",\"review\",\"genre\",\"notes\",\"track\",\"year\"}\n",
    "    csv_columns_to_join = [c for c in df.columns if c.lower() in likely] or [c for c in df.columns if df[c].dtype==\"object\"]\n",
    "\n",
    "    def row_to_text(row):\n",
    "        parts = []\n",
    "        for c in csv_columns_to_join:\n",
    "            val = row.get(c, \"\")\n",
    "            if pd.notna(val) and str(val).strip():\n",
    "                parts.append(f\"{c}: {str(val).strip()}\")\n",
    "        return \" | \".join(parts)\n",
    "\n",
    "    for i, r in df.iterrows():\n",
    "        add_record(row_to_text(r), f\"{cp.name}-{i}\")\n",
    "\n",
    "if not records:\n",
    "    raise SystemExit(\"‚ùå No records built from Top Shelf files. Check file contents/columns.\")\n",
    "\n",
    "print(f\"‚úÖ Built {len(records)} Top Shelf records.\")\n",
    "print(\"Example snippet:\\n\", records[0][\"text\"][:400])\n",
    "\n",
    "# ---------- 5) embed in memory ----------\n",
    "from openai import OpenAI\n",
    "client = OpenAI()  # reads OPENAI_API_KEY from env\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "MAX_DOCS = min(200, len(records))   # keep first run fast\n",
    "docs = records[:MAX_DOCS]\n",
    "texts = [d[\"text\"] for d in docs]\n",
    "ids   = [d[\"id\"]   for d in docs]\n",
    "\n",
    "def embed_texts(texts):\n",
    "    vecs = []\n",
    "    for t in texts:\n",
    "        e = client.embeddings.create(model=EMBEDDING_MODEL, input=t).data[0].embedding\n",
    "        vecs.append(np.array(e, dtype=np.float32))\n",
    "    return np.vstack(vecs)\n",
    "\n",
    "print(f\"üî¢ Embedding {len(texts)} docs‚Ä¶\")\n",
    "doc_vecs = embed_texts(texts)\n",
    "doc_vecs = doc_vecs / (np.linalg.norm(doc_vecs, axis=1, keepdims=True) + 1e-12)\n",
    "print(\"üìê Embeddings shape:\", doc_vecs.shape)\n",
    "\n",
    "# ---------- 6) ask one test question ----------\n",
    "CHAT_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "def top_k(query, k=5):\n",
    "    q = client.embeddings.create(model=EMBEDDING_MODEL, input=query).data[0].embedding\n",
    "    q = np.array(q, dtype=np.float32)\n",
    "    q = q / (np.linalg.norm(q) + 1e-12)\n",
    "    sims = doc_vecs @ q\n",
    "    idxs = np.argsort(-sims)[:k]\n",
    "    return [(ids[i], texts[i], float(sims[i])) for i in idxs]\n",
    "\n",
    "def answer_with_context(query, k=5, temperature=0.2):\n",
    "    hits = top_k(query, k=k)\n",
    "    context = \"\\n\\n\".join(f\"- {h[1]}\" for h in hits)\n",
    "    prompt = (\n",
    "        \"You are a precise assistant. Answer strictly using the CONTEXT below. \"\n",
    "        \"If the answer is not in the context, say you don't know.\\n\\n\"\n",
    "        f\"CONTEXT:\\n{context}\\n\\n\"\n",
    "        f\"QUESTION: {query}\\n\\nANSWER:\"\n",
    "    )\n",
    "    resp = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        temperature=temperature,\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip(), hits\n",
    "\n",
    "question = \"Name a notable artist or album and its genre from this Top Shelf collection, plus one distinctive detail.\"\n",
    "ans, hits = answer_with_context(question, k=5)\n",
    "print(\"\\nüß† ANSWER:\\n\", ans, \"\\n\")\n",
    "print(\"üîé TOP HITS:\")\n",
    "for hid, htxt, score in hits[:3]:\n",
    "    print(f\"- {hid} | score={score:.3f} | {htxt[:160]}‚Ä¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60bda15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c152c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /workspaces/suburban-disco/notebooks/notebooks\n",
      "Repo root: /workspaces/suburban-disco\n",
      ".env exists: True ‚Üí /workspaces/suburban-disco/.env\n",
      "data exists: True ‚Üí /workspaces/suburban-disco/data\n",
      "data files: ['top_rated_wines.csv', 'The_Top_Shelf_Collection_RAG_with_artist_years_filled_embeddings.jsonl', 'The_Top_Shelf_Collection_RAG_with_artist_years_filled.csv']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "cwd = Path.cwd()\n",
    "candidates = [cwd, *cwd.parents]\n",
    "repo_root = None\n",
    "for p in candidates:\n",
    "    if (p/\".env\").exists() or (p/\"README.md\").exists():\n",
    "        repo_root = p\n",
    "        break\n",
    "\n",
    "print(\"CWD:\", cwd)\n",
    "print(\"Repo root:\", repo_root)\n",
    "if repo_root:\n",
    "    print(\".env exists:\", (repo_root/\".env\").exists(), \"‚Üí\", repo_root/\".env\")\n",
    "    print(\"data exists:\", (repo_root/\"data\").exists(), \"‚Üí\", repo_root/\"data\")\n",
    "    if (repo_root/\"data\").exists():\n",
    "        print(\"data files:\", [f.name for f in (repo_root/\"data\").iterdir()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabda435",
   "metadata": {},
   "source": [
    "Verify key being read correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2c2aca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts with: sk-sv\n",
      "Total length: 167\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"Starts with:\", key[:5])\n",
    "print(\"Total length:\", len(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ed7a20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Notebook CWD: /workspaces/suburban-disco/notebooks/notebooks\n",
      "üì¶ Repo root: /workspaces/suburban-disco\n",
      "üß© .env exists: True ‚Üí /workspaces/suburban-disco/.env\n",
      "üìÅ data exists: True ‚Üí /workspaces/suburban-disco/data\n",
      "üìÑ data files: ['top_rated_wines.csv', 'The_Top_Shelf_Collection_RAG_with_artist_years_filled_embeddings.jsonl', 'The_Top_Shelf_Collection_RAG_with_artist_years_filled.csv']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# 1) Find repo root by walking up until we see a .env OR README.md\n",
    "cwd = Path.cwd()\n",
    "candidates = [cwd, *cwd.parents]\n",
    "repo_root = None\n",
    "for p in candidates:\n",
    "    if (p/\".env\").exists() or (p/\"README.md\").exists():\n",
    "        repo_root = p\n",
    "        break\n",
    "\n",
    "print(\"üîé Notebook CWD:\", cwd)\n",
    "print(\"üì¶ Repo root:\", repo_root)\n",
    "\n",
    "# 2) Show what we have\n",
    "if repo_root:\n",
    "    env_path = repo_root/\".env\"\n",
    "    data_dir = repo_root/\"data\"\n",
    "    print(\"üß© .env exists:\", env_path.exists(), \"‚Üí\", env_path)\n",
    "    print(\"üìÅ data exists:\", data_dir.exists(), \"‚Üí\", data_dir)\n",
    "    if data_dir.exists():\n",
    "        print(\"üìÑ data files:\", [f.name for f in data_dir.iterdir()])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Could not determine repo root. We‚Äôll assume cwd is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7410ead",
   "metadata": {},
   "source": [
    "Cell A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7387f6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Env loaded: True\n",
      "OpenAI key set: True\n"
     ]
    }
   ],
   "source": [
    "%pip -q install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "loaded = load_dotenv(repo_root/\".env\", override=True)\n",
    "print(\"Env loaded:\", loaded)\n",
    "print(\"OpenAI key set:\", bool(os.getenv(\"OPENAI_API_KEY\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "591538ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-<sk-svc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getenv(\"OPENAI_API_KEY\")[:10])  # just the first 10 chars for privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c86bd6",
   "metadata": {},
   "source": [
    "Cell A.1 - (re)install core packages (safe to re-run)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7121347d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script openai is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install --upgrade numpy pandas openai tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd22b39",
   "metadata": {},
   "source": [
    "Cell B1 ‚Äî build Top Shelf records only (drop-in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1aa02d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'repo_root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict\n\u001b[32m      4\u001b[39m records: List[Dict] = []\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m data_dir = \u001b[43mrepo_root\u001b[49m / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Only load files that look like \"Top Shelf\"\u001b[39;00m\n\u001b[32m      8\u001b[39m topshelf_json = \u001b[38;5;28msorted\u001b[39m([p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m data_dir.glob(\u001b[33m\"\u001b[39m\u001b[33m*.json\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtop\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m p.name.lower() \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mshelf\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m p.name.lower()])\n",
      "\u001b[31mNameError\u001b[39m: name 'repo_root' is not defined"
     ]
    }
   ],
   "source": [
    "import pathlib, json, pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "records: List[Dict] = []\n",
    "data_dir = repo_root / \"data\"\n",
    "\n",
    "# Only load files that look like \"Top Shelf\"\n",
    "topshelf_json = sorted([p for p in data_dir.glob(\"*.json\") if \"top\" in p.name.lower() and \"shelf\" in p.name.lower()])\n",
    "topshelf_csv  = sorted([p for p in data_dir.glob(\"*.csv\")  if \"top\" in p.name.lower() and \"shelf\" in p.name.lower()])\n",
    "\n",
    "def add_record(text: str, rid: str):\n",
    "    text = (text or \"\").strip()\n",
    "    if text:\n",
    "        records.append({\"id\": rid, \"text\": text})\n",
    "\n",
    "def join_item_fields(item: dict) -> str:\n",
    "    txt = (item.get(\"text\") or item.get(\"content\") or \"\").strip() if isinstance(item, dict) else \"\"\n",
    "    if txt:\n",
    "        return txt\n",
    "    if isinstance(item, dict):\n",
    "        parts = []\n",
    "        for k in sorted(item.keys()):\n",
    "            v = item[k]\n",
    "            if isinstance(v, (str, int, float)) and str(v).strip():\n",
    "                parts.append(f\"{k}: {str(v).strip()}\")\n",
    "        return \" | \".join(parts)\n",
    "    return str(item)\n",
    "\n",
    "used_files = []\n",
    "\n",
    "if topshelf_json:\n",
    "    for jp in topshelf_json:\n",
    "        used_files.append(jp.name)\n",
    "        data = json.loads(jp.read_text(encoding=\"utf-8\"))\n",
    "        assert isinstance(data, list) and data, f\"{jp.name} must be a non-empty JSON list\"\n",
    "        for i, item in enumerate(data):\n",
    "            add_record(join_item_fields(item), f\"{jp.name}-{i}\")\n",
    "\n",
    "elif topshelf_csv:\n",
    "    for cp in topshelf_csv:\n",
    "        used_files.append(cp.name)\n",
    "        try:\n",
    "            df2 = pd.read_csv(cp)\n",
    "        except Exception:\n",
    "            try:\n",
    "                df2 = pd.read_csv(cp, encoding=\"utf-8\")\n",
    "            except Exception:\n",
    "                df2 = pd.read_csv(cp, encoding=\"latin-1\")\n",
    "\n",
    "        # If you know exact column names, set them here:\n",
    "        csv_columns_to_join = None  # e.g., [\"artist\",\"album\",\"genre\",\"description\"]\n",
    "        if csv_columns_to_join is None:\n",
    "            likely = {\"artist\",\"album\",\"title\",\"description\",\"review\",\"genre\",\"notes\",\"track\",\"year\"}\n",
    "            csv_columns_to_join = [c for c in df2.columns if c.lower() in likely] or \\\n",
    "                                  [c for c in df2.columns if df2[c].dtype == \"object\"]\n",
    "\n",
    "        def row_to_text(row):\n",
    "            parts = []\n",
    "            for c in csv_columns_to_join:\n",
    "                val = row.get(c, \"\")\n",
    "                if pd.notna(val) and str(val).strip():\n",
    "                    parts.append(f\"{c}: {str(val).strip()}\")\n",
    "            return \" | \".join(parts)\n",
    "\n",
    "        for i, r in df2.iterrows():\n",
    "            add_record(row_to_text(r), f\"{cp.name}-{i}\")\n",
    "\n",
    "else:\n",
    "    raise FileNotFoundError(\"No Top Shelf files found. Rename to include 'top' and 'shelf', or adjust the filters.\")\n",
    "\n",
    "print(\"Using files:\", used_files)\n",
    "print(f\"Built {len(records)} Top Shelf records.\")\n",
    "print(\"Example:\\n\", (records[0]['text'][:500] if records else \"NO RECORDS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36045f80",
   "metadata": {},
   "source": [
    "Cell C ‚Äî embed in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c95a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install openai numpy tiktoken\n",
    "\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "client = OpenAI()  # uses OPENAI_API_KEY\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "MAX_DOCS = min(200, len(records))\n",
    "docs = records[:MAX_DOCS]\n",
    "doc_texts = [d[\"text\"] for d in docs]\n",
    "doc_ids   = [d[\"id\"]   for d in docs]\n",
    "\n",
    "def embed_texts(texts):\n",
    "    vecs = []\n",
    "    for t in texts:\n",
    "        e = client.embeddings.create(model=EMBEDDING_MODEL, input=t).data[0].embedding\n",
    "        vecs.append(np.array(e, dtype=np.float32))\n",
    "    return np.vstack(vecs)\n",
    "\n",
    "print(f\"Embedding {len(doc_texts)} docs‚Ä¶\")\n",
    "doc_vecs = embed_texts(doc_texts)\n",
    "doc_vecs = doc_vecs / (np.linalg.norm(doc_vecs, axis=1, keepdims=True) + 1e-12)\n",
    "print(\"Embeddings shape:\", doc_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f3710",
   "metadata": {},
   "source": [
    "Cell D: Ask RAG question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fb53dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      " A well known Jazz record in the collection is \"Kind of Blue\" by Miles Davis. \n",
      "\n",
      "TOP HITS:\n",
      "- The_Top_Shelf_Collection_RAG_with_artist_years_filled.csv-24 | score=0.476 | Artist: Django Reinhardt | Title: First Recordings | Genre: Jazz/Gypsy Jazz‚Ä¶\n",
      "- The_Top_Shelf_Collection_RAG_with_artist_years_filled.csv-34 | score=0.450 | Artist: Ella Fitzgerald | Title: sings the George and Ira Gershwin Song Book | Genre: Jazz/Vocal Standards‚Ä¶\n",
      "- The_Top_Shelf_Collection_RAG_with_artist_years_filled.csv-56 | score=0.444 | Artist: Miles Davis | Title: Nefertiti | Genre: Jazz‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "ans, hits = answer_with_context(\n",
    "    \"What's a well known Jazz record in the collection?\",\n",
    "    k=5\n",
    ")\n",
    "print(\"ANSWER:\\n\", ans, \"\\n\")\n",
    "print(\"TOP HITS:\")\n",
    "for hid, htxt, score in hits[:3]:\n",
    "    print(f\"- {hid} | score={score:.3f} | {htxt[:160]}‚Ä¶\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e3ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
